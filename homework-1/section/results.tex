\section{Results and Discussion}
\label{sec:results}
\begin{table}[h!]
    \begin{center}
        \caption{MAP and NCDG scores for all runs}
        \label{tab:all_scores}
        \begin{tabular}{|c|c||c|c|} 
            \hline
            \textbf{Index} & \textbf{Run} & \textbf{MAP Score} & \textbf{NCDG Score}\\
            \hline\hline
            01 & en\_en & \cellcolor{red!30!white}0.0700 & \cellcolor{red!30!white}0.1614 \\
            \hline
            02 & en\_en\_3gram & 0.0704 & 0.1661 \\
            \hline
            03 & en\_en\_4gram & 0.0874 & 0.2025 \\
            \hline
            04 & en\_en\_5gram & 0.1028 & 0.2288 \\
            \hline
            05 & en\_en\_fr\_5gram & \cellcolor{red!60!white}0.0669 & \cellcolor{red!60!white}0.1525 \\
            \hline
            06 & en\_en\_4gram\_ner & \cellcolor{red}0.0360 & \cellcolor{red}0.1098 \\
            \hline
            07 & fr\_fr & 0.1656 & 0.3135 \\
            \hline
            08 & fr\_fr\_3gram & \cellcolor{green!30!white}0.1698 & \cellcolor{green!30!white}0.3208 \\
            \hline
            09 & fr\_fr\_4gram & \cellcolor{green!60!white}0.1737 & \cellcolor{green!60!white}0.3269 \\
            \hline
            10 & fr\_fr\_5gram & \cellcolor{green}0.1748 & \cellcolor{green}0.3285 \\
            \hline
            11 & fr\_en\_fr\_5gram & 0.1288 & 0.2797 \\
            \hline
            12 & fr\_fr\_4gram\_ner & 0.1362 & 0.2881 \\
            \hline
        \end{tabular}
    \end{center}
\end{table}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\textwidth]{figure/allScores.png}
	\caption{All scores sorted by MAP score}
	\label{fig:sorted_scores}
\end{figure}
The analysis shows that the highest MAP score (0.1748) is achieved by \texttt{fr\_fr\_5gram}, followed by \texttt{fr\_fr\_4gram} (0.1737) and \texttt{fr\_fr\_3gram} (0.1698), while the lowest MAP score (0.0360) is obtained by \texttt{en\_en\_4gram\_ner}.
Similarly, the highest NDCG score (0.3208) belongs to \texttt{fr\_fr\_4gram\_ner}, followed by \texttt{fr\_fr\_5gram} (0.3285) and \texttt{fr\_fr\_4gram} (0.3269), whereas the lowest NDCG score (0.1098) corresponds to \texttt{en\_en\_4gram\_ner}.\\
Results suggest that French queries perform better than their English counterparts, possibly due to the training data's French origin and later translation into English. Moreover, the IR system's effectiveness generally increases with a larger N-gram size, as indicated by the higher scores
of \texttt{en\_en\_5gram} and \texttt{fr\_fr\_5gram}. Conversely, the inclusion of NER in the indexing process seems to have a negative impact on the scores, as shown by the lower scores of \texttt{en\_en\_4gram\_ner} and \texttt{fr\_fr\_4gram\_ner}.
The use of query expansion with synonyms in English does not seem to improve the search results to any great extent.\\
Here we can see a chart ranking of the five best scores, they are the runs that have been presented at CLEF:
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\textwidth]{figure/bestScores.png}
	\caption{Best MAP and NDCG scores}
	\label{fig:best_scores}
\end{figure}
It's interesting to notice that the cross-language approaches (\texttt{en\_en\_fr\_5gram} and \texttt{fr\_en\_fr\_5gram}) are out of the five bests systems.
It turns out that searching for English words in French documents and vice versa messes up the search, lowering the score.
Another interesting aspect is that the worst-performing index is the one with named entity recognition in English (\texttt{en\_en\_4gram\_ner}): it combines translated queries and NER, which appears to be the two worst-performing approaches.\\
In general, we focus more on trying multiple approaches, this is why our score has such a big space for improvement.
As already said, French queries with bigger N-gram sizes perform better.
Instead of relying on single-word matches, the queries could take place with more context, resulting in better search
results.\\
Following the competition workflow, we created the indexes based on the test data and re-executed the top five
runs (see Figure~\ref{fig:best_scores}). These runs will be the ones delivered to CLEF.